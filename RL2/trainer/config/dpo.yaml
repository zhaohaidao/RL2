data:
  path: null
  max_length: null
  batch_size: 128

actor:
  model_name: null
  use_liger_kernel: false
  gradient_checkpointing: true
  ddp_size: 1
  tp_size: 1
  sp_size: 1
  optimizer_dir: null
  max_length_per_device: null
  beta: 0.1
  lr: 5e-7
  weight_decay: 1e-2
  max_grad_norm: 1.0
  warmup_ratio: 0.1
  offload_optimizer: true
  save_dir: ckpts/${trainer.experiment_name}
  save_freq: null
  save_optimizer: true

  lora:
    rank: 0
    alpha: 16
    target_modules: all-linear
    dropout: 0

ref_actor:
  model_name: ${actor.model_name}
  use_liger_kernel: ${actor.use_liger_kernel}
  ddp_size: ${actor.ddp_size}
  tp_size: ${actor.tp_size}
  sp_size: ${actor.sp_size}
  max_inference_length_per_device: ${actor.max_length_per_device}
  offload_model: false

trainer:
  project: null
  experiment_name: null
  n_epochs: 1
  disable_wandb: false